{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannisthamaiti/DiffusionModels_DDPM_DDIM/blob/main/transformers_doc/en/pytorch/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "b5l3_35W_ib4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84ec969-2414-4f0a-8dfb-f0fbe55be15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9_Mtj55_ib9"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yFDmQkw_ib-"
      },
      "source": [
        "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. 🤗 Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you'll learn that for:\n",
        "\n",
        "* Text, use a [Tokenizer](https://huggingface.co/docs/transformers/main/en/./main_classes/tokenizer) to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "`AutoProcessor` **always** works and automatically chooses the correct class for the model you're using, whether you're using a tokenizer, image processor, feature extractor or processor.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Before you begin, install 🤗 Datasets so you can load some datasets to experiment with:\n",
        "\n",
        "```bash\n",
        "pip install datasets\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "hide_input": true,
        "id": "0gN2tVuc_icA",
        "outputId": "ba1ac7ba-d121-4356-a9eb-d6e2d6d96656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Yffk5aydLzg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Yffk5aydLzg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "\n",
        "sequence = \"A Titan RTX has 24GB of VRAM\""
      ],
      "metadata": {
        "id": "Qk_F7GtVBaxD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAGKzsiG_icA"
      },
      "source": [
        "## Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wes1r3th_icC"
      },
      "source": [
        "The main tool for preprocessing textual data is a [tokenizer](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer). A tokenizer splits text into *tokens* according to a set of rules. The tokens are converted into numbers and then tensors, which become the model inputs. Any additional inputs required by the model are added by the tokenizer.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "If you plan on using a pretrained model, it's important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referred to as the *vocab*) during pretraining.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "We are using wordpiece tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "print(tokenized_sequence)\n",
        "inputs = tokenizer(sequence)\n",
        "encoded_sequence = inputs[\"input_ids\"]\n",
        "print(encoded_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXiSvW6KJewR",
        "outputId": "4714fbad-9d99-41c4-b249-d3c3fb0049b9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']\n",
            "[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The token indices are under the key input_ids:"
      ],
      "metadata": {
        "id": "z_K08AVEJ0Jt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW4QnZsL_icD"
      },
      "source": [
        "Then pass your text to the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sequence = tokenizer.decode(encoded_sequence)"
      ],
      "metadata": {
        "id": "qEvd54miJ6hf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqVF92V1KCKI",
        "outputId": "24fffbd1-0e37-4a3c-895a-578668e4fe1f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] A Titan RTX has 24GB of VRAM [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "SX8oWQW0_icD",
        "outputId": "77f49117-5697-4e99-cbc0-f3c123c55d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2091, 1136, 1143, 13002, 1107, 1103, 5707, 1104, 16678, 1116, 117, 1111, 1152, 1132, 11515, 1105, 3613, 1106, 4470, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the tokenizer automatically adds “special tokens” (if the associated model relies on them) which are special IDs the model sometimes uses.\n",
        "\n",
        "If we decode the previous sequence of ids,"
      ],
      "metadata": {
        "id": "yODJvN7JJ7yC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHFE1JnA_icE"
      },
      "source": [
        "The tokenizer returns a dictionary with three important items:\n",
        "\n",
        "* [input_ids](https://huggingface.co/docs/transformers/main/en/glossary#input-ids) are the indices corresponding to each token in the sentence.\n",
        "* [attention_mask](https://huggingface.co/docs/transformers/main/en/glossary#attention-mask) indicates whether a token should be attended to or not.\n",
        "* [token_type_ids](https://huggingface.co/docs/transformers/main/en/glossary#token-type-ids) identifies which sequence a token belongs to when there is more than one sequence.\n",
        "\n",
        "Return your input by decoding the `input_ids`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PisZpBZ0_icE",
        "outputId": "29291d79-073e-427f-cf84-a608530209b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TipnBgj_icE"
      },
      "source": [
        "As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separator) - to the sentence. Not all models need\n",
        "special tokens, but if they do, the tokenizer automatically adds them for you.\n",
        "\n",
        "If there are several sentences you want to preprocess, pass them as a list to the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WwjPOmPC_icF",
        "outputId": "38e639f1-6306-4ef9-f86b-0c99d8cec69f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_inputs = tokenizer(batch_sentences)\n",
        "print(encoded_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oDDNfLD_icF"
      },
      "source": [
        "### Pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0yFRGGl_icF"
      },
      "source": [
        "Sentences aren't always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special *padding token* to shorter sentences.\n",
        "\n",
        "Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "P8Bpw2y-_icG",
        "outputId": "ed74ac16-cea3-4bcc-bf6c-3fba8ef3a783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True)\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8wbUXme_icG"
      },
      "source": [
        "The first and third sentences are now padded with `0`'s because they are shorter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijsGCUYF_icG"
      },
      "source": [
        "### Truncation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUbHhqM6_icG"
      },
      "source": [
        "On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you'll need to truncate the sequence to a shorter length.\n",
        "\n",
        "Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "eXJKUyka_icG",
        "outputId": "7d3392c6-b36e-4dc4-8bad-c39477017dcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfBEIoEV_icH"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "Check out the [Padding and truncation](https://huggingface.co/docs/transformers/main/en/./pad_truncation) concept guide to learn more different padding and truncation arguments.\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSNRGhA6_icH"
      },
      "source": [
        "### Build tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0qnUEeS_icH"
      },
      "source": [
        "Finally, you want the tokenizer to return the actual tensors that get fed to the model.\n",
        "\n",
        "Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "44vpu3j5_icH",
        "outputId": "54a3408c-6a8e-4028-af07-5b7e47c00015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1252,  1184,  1164,  1248,  6462,   136,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0],\n",
            "        [  101,  1790,   112,   189,  1341,  1119,  3520,  1164,  1248,  6462,\n",
            "           117, 21902,  1643,   119,   102],\n",
            "        [  101,  1327,  1164,  5450, 23434,   136,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "batch_sentences = [\n",
        "    \"But what about second breakfast?\",\n",
        "    \"Don't think he knows about second breakfast, Pip.\",\n",
        "    \"What about elevensies?\",\n",
        "]\n",
        "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another variation sentencepiece tokenization"
      ],
      "metadata": {
        "id": "18YygAB7L7oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLNetTokenizer\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
        "tokenized_sequence = tokenizer.tokenize(sequence)\n",
        "print(tokenized_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAwdlr9lMAir",
        "outputId": "dabf43cf-ba58-4a8c-98c0-7a7bbc60551e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁A', '▁Titan', '▁R', 'TX', '▁has', '▁24', 'GB', '▁of', '▁V', 'RAM']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}