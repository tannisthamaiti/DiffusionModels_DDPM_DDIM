{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "from diffusion_utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):  # cfeat - context features\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # number of input channels, number of intermediate feature maps and number of classes\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_cfeat = n_cfeat\n",
    "        self.h = height  #assume h == w. must be divisible by 4, so 28,24,20,16...\n",
    "\n",
    "        # Initialize the initial convolutional layer\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        \n",
    "         # original: self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d((3)), nn.GELU())\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
    "        self.timeembed1 = EmbedFC(1, 8*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 8*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 8*n_feat)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=4)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "       \n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "\n",
    "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            nn.GroupNorm(8, n_feat), # normalize\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        x : (batch, n_feat, h, w) : input image\n",
    "        t : (batch, n_cfeat)      : time step\n",
    "        c : (batch, n_classes)    : context label\n",
    "        \"\"\"\n",
    "        # x is the input image, c is the context label, t is the timestep, context_mask says which samples to block the context on\n",
    "\n",
    "        # pass the input image through the initial convolutional layer\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "        print(xp1.shape)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "        print(xp2.shape)\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "        print(xp3.shape)\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "        xp5 = self.pool4(xp4)\n",
    "        print(f\"xp5 {xp5.shape}\")\n",
    "        hiddenvec = self.to_vec(xp5)\n",
    "        print(f\"hiddenvec {hiddenvec.shape}\")\n",
    "        # convert the feature maps to a vector and apply an activation\n",
    "        up1 = self.upconv1(hiddenvec)\n",
    "        print(f\"up1 {up1.shape}\")\n",
    "        \n",
    "       \n",
    "        # mask out context if context_mask == 1\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "        print(f\"c {self.contextembed1(c).shape}\")\n",
    "        # embed context and timestep\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 8, 1, 1)     # (batch, 2*n_feat, 1,1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 8, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        print(f\"uunet forward: cemb1 {cemb1.shape}. temb1 {temb1.shape}, cemb2 {cemb2.shape}. temb2 {temb2.shape}\")\n",
    "        #print(cemb1*up1 + temb1)\n",
    "        \n",
    "        up2 = self.up1(cemb1*up1 + temb1, xp5)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2 + temb2, xp4)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 128 # 16x16 image\n",
    "save_dir = 'weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 32\n",
    "n_epoch = 100\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 1, emb_dim 512 \n",
      "input_dim 1, emb_dim 64 \n",
      "input_dim 5, emb_dim 512 \n",
      "input_dim 5, emb_dim 512 \n"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprite shape: (70257, 128, 128, 3)\n",
      "labels shape: (70257, 5)\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "dataset = CustomDataset(\"./wind_366X366.npy\", \"./wind_label_366X366.npy\", transform, null_context=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 64, 64])\n",
      "torch.Size([32, 128, 32, 32])\n",
      "torch.Size([32, 256, 16, 16])\n",
      "xp5 torch.Size([32, 512, 4, 4])\n",
      "hiddenvec torch.Size([32, 512, 1, 1])\n",
      "up1 torch.Size([32, 512, 4, 4])\n",
      "c torch.Size([32, 512])\n",
      "uunet forward: cemb1 torch.Size([32, 512, 1, 1]). temb1 torch.Size([32, 512, 1, 1]), cemb2 torch.Size([256, 64, 1, 1]). temb2 torch.Size([32, 64, 1, 1])\n",
      "tensor([[[[ 0.2336,  0.2326,  0.2362,  0.2342],\n",
      "          [ 0.2326,  0.2352,  0.2344,  0.2351],\n",
      "          [ 0.2328,  0.2331,  0.2334,  0.2340],\n",
      "          [ 0.2336,  0.2352,  0.2339,  0.2323]],\n",
      "\n",
      "         [[ 0.2319,  0.2320,  0.2318,  0.2326],\n",
      "          [ 0.2325,  0.2324,  0.2325,  0.2333],\n",
      "          [ 0.2312,  0.2322,  0.2318,  0.2324],\n",
      "          [ 0.2324,  0.2315,  0.2324,  0.2335]],\n",
      "\n",
      "         [[-0.0753, -0.0713, -0.0712, -0.0753],\n",
      "          [-0.0731, -0.0774, -0.0757, -0.0752],\n",
      "          [-0.0737, -0.0723, -0.0753, -0.0761],\n",
      "          [-0.0778, -0.0720, -0.0745, -0.0733]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3549, -0.3549, -0.3549, -0.3549],\n",
      "          [-0.3549, -0.3549, -0.3549, -0.3549],\n",
      "          [-0.3549, -0.3549, -0.3549, -0.3549],\n",
      "          [-0.3549, -0.3549, -0.3549, -0.3549]],\n",
      "\n",
      "         [[ 0.0624,  0.0580,  0.0635,  0.0631],\n",
      "          [ 0.0612,  0.0653,  0.0631,  0.0595],\n",
      "          [ 0.0616,  0.0599,  0.0586,  0.0636],\n",
      "          [ 0.0617,  0.0627,  0.0617,  0.0604]],\n",
      "\n",
      "         [[ 0.3417,  0.3420,  0.3401,  0.3404],\n",
      "          [ 0.3411,  0.3404,  0.3405,  0.3388],\n",
      "          [ 0.3392,  0.3412,  0.3395,  0.3404],\n",
      "          [ 0.3405,  0.3413,  0.3392,  0.3415]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3556,  0.3556,  0.3559,  0.3557],\n",
      "          [ 0.3556,  0.3559,  0.3557,  0.3558],\n",
      "          [ 0.3555,  0.3556,  0.3556,  0.3557],\n",
      "          [ 0.3556,  0.3558,  0.3557,  0.3555]],\n",
      "\n",
      "         [[ 0.3612,  0.3612,  0.3612,  0.3613],\n",
      "          [ 0.3613,  0.3613,  0.3613,  0.3614],\n",
      "          [ 0.3611,  0.3613,  0.3612,  0.3613],\n",
      "          [ 0.3613,  0.3612,  0.3613,  0.3614]],\n",
      "\n",
      "         [[-0.1081, -0.1077, -0.1076, -0.1081],\n",
      "          [-0.1080, -0.1083, -0.1082, -0.1082],\n",
      "          [-0.1079, -0.1079, -0.1081, -0.1082],\n",
      "          [-0.1084, -0.1078, -0.1081, -0.1079]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5562, -0.5562, -0.5562, -0.5562],\n",
      "          [-0.5562, -0.5562, -0.5562, -0.5562],\n",
      "          [-0.5562, -0.5562, -0.5562, -0.5562],\n",
      "          [-0.5562, -0.5562, -0.5562, -0.5562]],\n",
      "\n",
      "         [[-0.2149, -0.2154, -0.2146, -0.2147],\n",
      "          [-0.2149, -0.2145, -0.2148, -0.2150],\n",
      "          [-0.2149, -0.2150, -0.2153, -0.2146],\n",
      "          [-0.2149, -0.2148, -0.2149, -0.2150]],\n",
      "\n",
      "         [[ 0.3110,  0.3110,  0.3108,  0.3108],\n",
      "          [ 0.3109,  0.3109,  0.3108,  0.3106],\n",
      "          [ 0.3107,  0.3109,  0.3107,  0.3108],\n",
      "          [ 0.3109,  0.3109,  0.3107,  0.3110]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2664,  0.2659,  0.2680,  0.2668],\n",
      "          [ 0.2660,  0.2674,  0.2669,  0.2674],\n",
      "          [ 0.2659,  0.2662,  0.2664,  0.2667],\n",
      "          [ 0.2664,  0.2674,  0.2666,  0.2656]],\n",
      "\n",
      "         [[ 0.2639,  0.2640,  0.2639,  0.2643],\n",
      "          [ 0.2643,  0.2642,  0.2643,  0.2648],\n",
      "          [ 0.2635,  0.2641,  0.2639,  0.2642],\n",
      "          [ 0.2642,  0.2637,  0.2642,  0.2649]],\n",
      "\n",
      "         [[-0.0881, -0.0857, -0.0854, -0.0880],\n",
      "          [-0.0867, -0.0892, -0.0883, -0.0882],\n",
      "          [-0.0869, -0.0861, -0.0881, -0.0884],\n",
      "          [-0.0895, -0.0859, -0.0876, -0.0870]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4110, -0.4110, -0.4110, -0.4110],\n",
      "          [-0.4110, -0.4110, -0.4110, -0.4110],\n",
      "          [-0.4110, -0.4110, -0.4110, -0.4110],\n",
      "          [-0.4110, -0.4110, -0.4110, -0.4110]],\n",
      "\n",
      "         [[-0.0076, -0.0105, -0.0069, -0.0073],\n",
      "          [-0.0085, -0.0060, -0.0072, -0.0092],\n",
      "          [-0.0082, -0.0092, -0.0099, -0.0070],\n",
      "          [-0.0081, -0.0073, -0.0078, -0.0089]],\n",
      "\n",
      "         [[ 0.3179,  0.3182,  0.3169,  0.3173],\n",
      "          [ 0.3176,  0.3173,  0.3173,  0.3161],\n",
      "          [ 0.3164,  0.3175,  0.3167,  0.3172],\n",
      "          [ 0.3172,  0.3177,  0.3165,  0.3178]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3577,  0.3577,  0.3581,  0.3578],\n",
      "          [ 0.3577,  0.3580,  0.3579,  0.3579],\n",
      "          [ 0.3577,  0.3577,  0.3578,  0.3578],\n",
      "          [ 0.3578,  0.3579,  0.3578,  0.3576]],\n",
      "\n",
      "         [[ 0.3636,  0.3636,  0.3636,  0.3637],\n",
      "          [ 0.3637,  0.3637,  0.3637,  0.3638],\n",
      "          [ 0.3635,  0.3636,  0.3636,  0.3637],\n",
      "          [ 0.3637,  0.3636,  0.3637,  0.3638]],\n",
      "\n",
      "         [[-0.1083, -0.1078, -0.1078, -0.1083],\n",
      "          [-0.1081, -0.1085, -0.1083, -0.1083],\n",
      "          [-0.1081, -0.1080, -0.1083, -0.1083],\n",
      "          [-0.1085, -0.1079, -0.1082, -0.1080]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5596, -0.5596, -0.5596, -0.5596],\n",
      "          [-0.5596, -0.5596, -0.5596, -0.5596],\n",
      "          [-0.5596, -0.5596, -0.5596, -0.5596],\n",
      "          [-0.5596, -0.5596, -0.5596, -0.5596]],\n",
      "\n",
      "         [[-0.2201, -0.2206, -0.2199, -0.2200],\n",
      "          [-0.2201, -0.2198, -0.2200, -0.2203],\n",
      "          [-0.2201, -0.2203, -0.2206, -0.2199],\n",
      "          [-0.2202, -0.2201, -0.2202, -0.2203]],\n",
      "\n",
      "         [[ 0.3119,  0.3118,  0.3116,  0.3117],\n",
      "          [ 0.3117,  0.3117,  0.3117,  0.3115],\n",
      "          [ 0.3115,  0.3118,  0.3115,  0.3117],\n",
      "          [ 0.3117,  0.3118,  0.3115,  0.3118]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2932,  0.2927,  0.2944,  0.2934],\n",
      "          [ 0.2926,  0.2940,  0.2935,  0.2939],\n",
      "          [ 0.2927,  0.2930,  0.2930,  0.2934],\n",
      "          [ 0.2931,  0.2940,  0.2933,  0.2925]],\n",
      "\n",
      "         [[ 0.2919,  0.2920,  0.2920,  0.2923],\n",
      "          [ 0.2923,  0.2922,  0.2923,  0.2927],\n",
      "          [ 0.2916,  0.2921,  0.2919,  0.2922],\n",
      "          [ 0.2922,  0.2917,  0.2922,  0.2928]],\n",
      "\n",
      "         [[-0.0968, -0.0947, -0.0947, -0.0968],\n",
      "          [-0.0957, -0.0980, -0.0970, -0.0968],\n",
      "          [-0.0959, -0.0952, -0.0967, -0.0971],\n",
      "          [-0.0982, -0.0950, -0.0963, -0.0957]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4557, -0.4557, -0.4557, -0.4557],\n",
      "          [-0.4557, -0.4557, -0.4557, -0.4557],\n",
      "          [-0.4557, -0.4557, -0.4557, -0.4557],\n",
      "          [-0.4557, -0.4557, -0.4557, -0.4557]],\n",
      "\n",
      "         [[-0.0670, -0.0693, -0.0666, -0.0668],\n",
      "          [-0.0676, -0.0654, -0.0666, -0.0684],\n",
      "          [-0.0673, -0.0680, -0.0690, -0.0665],\n",
      "          [-0.0673, -0.0668, -0.0675, -0.0679]],\n",
      "\n",
      "         [[ 0.3072,  0.3074,  0.3064,  0.3066],\n",
      "          [ 0.3069,  0.3065,  0.3066,  0.3057],\n",
      "          [ 0.3059,  0.3070,  0.3061,  0.3066],\n",
      "          [ 0.3066,  0.3071,  0.3059,  0.3071]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2552,  0.2543,  0.2574,  0.2558],\n",
      "          [ 0.2543,  0.2568,  0.2559,  0.2565],\n",
      "          [ 0.2545,  0.2547,  0.2549,  0.2556],\n",
      "          [ 0.2552,  0.2566,  0.2555,  0.2542]],\n",
      "\n",
      "         [[ 0.2525,  0.2526,  0.2525,  0.2533],\n",
      "          [ 0.2532,  0.2530,  0.2531,  0.2538],\n",
      "          [ 0.2519,  0.2528,  0.2524,  0.2530],\n",
      "          [ 0.2530,  0.2522,  0.2529,  0.2540]],\n",
      "\n",
      "         [[-0.0839, -0.0804, -0.0805, -0.0839],\n",
      "          [-0.0821, -0.0859, -0.0842, -0.0839],\n",
      "          [-0.0827, -0.0815, -0.0839, -0.0847],\n",
      "          [-0.0863, -0.0810, -0.0831, -0.0820]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3920, -0.3920, -0.3920, -0.3920],\n",
      "          [-0.3920, -0.3920, -0.3920, -0.3920],\n",
      "          [-0.3920, -0.3920, -0.3920, -0.3920],\n",
      "          [-0.3920, -0.3920, -0.3920, -0.3920]],\n",
      "\n",
      "         [[ 0.0168,  0.0129,  0.0177,  0.0174],\n",
      "          [ 0.0158,  0.0194,  0.0175,  0.0145],\n",
      "          [ 0.0163,  0.0147,  0.0132,  0.0177],\n",
      "          [ 0.0163,  0.0171,  0.0160,  0.0151]],\n",
      "\n",
      "         [[ 0.3249,  0.3251,  0.3235,  0.3237],\n",
      "          [ 0.3244,  0.3235,  0.3238,  0.3225],\n",
      "          [ 0.3227,  0.3245,  0.3229,  0.3237],\n",
      "          [ 0.3239,  0.3245,  0.3226,  0.3248]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2196 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [256, 64, 2, 2], expected input[32, 1024, 4, 4] to have 256 channels, but got 1024 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1087406/3382421487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# use network to recover noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mpred_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_noise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1087406/203884614.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t, c)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcemb1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mup1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m#up1 = self.up0(hiddenvec)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mup2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcemb1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mup1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add and multiply embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mup3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcemb2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mup2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cymach/diffusion_models/DiffusionModels_DDPM_DDIM/diffusion_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Pass the concatenated tensor through the sequential model and return the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    950\u001b[0m             num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m    953\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [256, 64, 2, 2], expected input[32, 1024, 4, 4] to have 256 channels, but got 1024 channels instead"
     ]
    }
   ],
   "source": [
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    \n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    pbar = tqdm(dataloader, mininterval=2 )\n",
    "    for x, _ in pbar:   # x: images\n",
    "        optim.zero_grad()\n",
    "        x = x.float().to(device)\n",
    "        \n",
    "        \n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) \n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "        #print(noise.shape)\n",
    "        \n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps)\n",
    "        print(pred_noise.shape)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bop",
   "language": "python",
   "name": "bop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
